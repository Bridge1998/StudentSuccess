import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
from sklearn import datasets,model_selection,linear_model,metrics,decomposition
import seaborn as sns

df = pd.read_csv("UF_R3_ANALYSIS_LASTMAJOR0714BE.csv")

df.dtypes
df.isnull().sum()
df.dropna(axis=0, how='any',inplace=True)
df['STUDENT_SUCCESSNO'].value_counts()
 
 
Categorical variables：


cat = df[["JUNIOR_SENIOR_FLAGNO",
"ENRL_CNT",
"MAJORCOUNT",
"UF_CLASSNO",
"RESIDENCYNO",
"TERM_END_DT_SID_CATGRYNO",
"TERM_BEG_DT_CATGRYNO",
"LOW_TERM_GPA_IND",
"PARTTIME_TERM_IND",
"NOT_REG_TERM_IND",
"WITHDRWL_TERM_IND",
"FULLTIME_TERM_IND",
"OVR_12HR_TERM_IND"]]
corcat = cat.corr(method='kendall')
f, ax = plt.subplots(figsize=(15,15))
sns.heatmap(corcat, vmax=.8, annot=True, square=True)
plt.show()

con = df[[
"TERM_SID",
"TERM_BEG_DT_SID",
"TERM_END_DT_SID",
"AGE_YEARS",
"AGE_MONTHS",
"AGE_DAYS",
"TOT_TRNSFR",
"TOT_TEST_CREDIT",
"UNT_TAKEN_PRGRSS",
"CUM_GPA",
"TOT_TAKEN_GPA",
“UNT_TAKEN_GPA”
]]corcon = con.corr(method='pearson')
f, ax = plt.subplots(figsize=(15,15))
sns.heatmap(corcon, vmax=.8, annot=True, square=True)
plt.show()

Create Two Variables: 'NOT_GRADUATED','GRADUATED_IN_4':
df['NOT_GRADUATED']=df['STUDENT_SUCCESSNO']
df['GRADUATED_IN_4']=df['STUDENT_SUCCESSNO']
df['NOT_GRADUATED'].loc[df['NOT_GRADUATED'] !=1]=0
df['GRADUATED_IN_4'].loc[df['GRADUATED_IN_4'] !=2]=0
df['GRADUATED_IN_4'].loc[df['GRADUATED_IN_4'] ==2]=1


Draw Correlation Maps:
con = df[[
"UF_CLASSNO",
"RESIDENCYNO",
"UF_CLASS_EOTNO",
"JUNIOR_SENIOR_FLAGNO",
"ENRL_CNT",
"TOT_TAKEN_GPA",'LOW_TERM_GPA_IND','NOT_GRADUATED',
'GRADUATED_IN_4']]
corcon = con.corr(method='pearson')
f, ax = plt.subplots(figsize=(15,15))
sns.heatmap(corcon, vmax=.8, annot=True, square=True)
plt.show()

Draw Correlation Maps:
con = df[[
"UF_CLASSNO",
"RESIDENCYNO",
"UF_CLASS_EOTNO",
"JUNIOR_SENIOR_FLAGNO",
"ENRL_CNT",
"TOT_TAKEN_GPA",'LOW_TERM_GPA_IND','NOT_GRADUATED',
'GRADUATED_IN_4']]
corcon = con.corr(method='pearson')
f, ax = plt.subplots(figsize=(15,15))
sns.heatmap(corcon, vmax=.8, annot=True, square=True)
plt.show()

Draw Correlation Maps:
con = df[[
"MAJORCOUNT",
"WITHDRWL_TERM_IND",
"PARTTIME_TERM_IND",
"TOT_TAKEN_GPA",'NOT_GRADUATED',
'GRADUATED_IN_4']]
corcon = con.corr(method='pearson')
f, ax = plt.subplots(figsize=(15,15))
sns.heatmap(corcon, vmax=.8, annot=True, square=True)
plt.show()


Draw Correlation Maps:
con = df[[
"FULLTIME_TERM_IND",
"WITHDRWL_TERM_IND",
"PARTTIME_TERM_IND",
    'RESIDENCYNO',
"TOT_TAKEN_GPA",'NOT_GRADUATED',
'GRADUATED_IN_4']]
corcon = con.corr(method='pearson')
f, ax = plt.subplots(figsize=(15,15))
sns.heatmap(corcon, vmax=.8, annot=True, square=True)
plt.show










from sklearn.model_selection import train_test_split
Y = df['STUDENT_SUCCESSNO']
X = df.drop(['STUDENT_SUCCESSNO', 'TERM_END_DT_SID','TERM_SID','UNT_TAKEN_GPA'], axis= 1)
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1234)
 
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
RF = RandomForestClassifier()
RF = RF.fit(x_train, y_train)
y_pred = RF.predict(x_test)
print (metrics.classification_report(y_test, y_pred))
Y_score = RF.predict_proba(x_test)[:,1]

from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, y_pred)


from sklearn.neighbors import KNeighborsClassifier as kNN
model = kNN(n_neighbors = 3, algorithm='auto', weights='distance')
model.fit(x_train,y_train)
y_pred = model.predict(x_test)
print('KNN')
print('acc:', accuracy_score(y_test, y_pred))
print('rec:', recall_score(y_test, y_pred))
print('F1:', f1_score(y_test, y_pred))

from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, y_pred)



Cross Validation
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
from sklearn.model_selection import cross_val_score

Y = df['STUDENT_SUCCESSNO']
X = df.drop(['STUDENT_SUCCESSNO'], axis= 1)
RF = RandomForestClassifier()
scores = cross_val_score(RF, X, Y, cv=5)
scores
 

 
 
Cross Validation
from sklearn.metrics import accuracy_score, recall_score, f1_score
from sklearn.neighbors import KNeighborsClassifier as kNN

model = kNN(n_neighbors = 3, algorithm='auto', weights='distance')
Y = df['STUDENT_SUCCESSNO']
X = df.drop(['STUDENT_SUCCESSNO'], axis= 1)
scores = cross_val_score(model, X, Y, cv=5)
scores
 

Visualization
fig = plt.figure(figsize=(20,10))
plt.bar(df['STUDENT_SUCCESSNO'].value_counts().index, df['STUDENT_SUCCESSNO'].value_counts().values)
plt.xticks(df['STUDENT_SUCCESSNO'].value_counts().index,fontsize=15)
plt.show() 
plt.scatter(X.values[:, 0], X.values[:, 1], c=Y)
plt.show()
 




LIME
 
dfcolumn = df.columns.values.tolist()
dfcolumn.remove('STUDENT_SUCCESSNO')
dfcolumn.remove('TERM_END_DT_SID')
dfcolumn.remove('TERM_SID')
dfcolumn.remove('UNT_TAKEN_GPA')
 
import lime
import lime.lime_tabular
xtrainnp = np.array(x_train)
ytrainnp = np.array(y_train)
 
explainer = lime.lime_tabular.LimeTabularExplainer(xtrainnp,
                	feature_names=dfcolumn,
                	class_names=['1','2','3'],
                	verbose=True, mode='regression')
choosen_instance = x_test.iloc[[200]].values[0]
exp = explainer.explain_instance(choosen_instance, RF.predict, num_features=24)
exp.show_in_notebook(show_all=False)
 
 
choosen_instance = x_test.iloc[[3]].values[0]
exp = explainer.explain_instance(choosen_instance, RF.predict, num_features=24)
exp.show_in_notebook(show_all=False)
 
One-hot encoding
 
df_JUNIOR_SENIOR_FLAGNO = df['JUNIOR_SENIOR_FLAGNO'].apply(str).str.get_dummies().add_prefix('JUNIOR_SENIOR_FLAGNO: ')
df_ENRL_CNT = df['ENRL_CNT'].apply(str).str.get_dummies().add_prefix('ENRL_CNT: ')
df_MAJORCOUNT = df['MAJORCOUNT'].apply(str).str.get_dummies().add_prefix('MAJORCOUNT: ')
df_UF_CLASSNO = df['UF_CLASSNO'].apply(str).str.get_dummies().add_prefix('UF_CLASSNO: ')
df_RESIDENCYNO = df['RESIDENCYNO'].apply(str).str.get_dummies().add_prefix('RESIDENCYNO: ')
df_TERM_END_DT_SID_CATGRYNO = df['TERM_END_DT_SID_CATGRYNO'].apply(str).str.get_dummies().add_prefix('TERM_END_DT_SID_CATGRYNO: ')
df_TERM_BEG_DT_CATGRYNO = df['TERM_BEG_DT_CATGRYNO'].apply(str).str.get_dummies().add_prefix('TERM_BEG_DT_CATGRYNO: ')
df_LOW_TERM_GPA_IND = df['LOW_TERM_GPA_IND'].apply(str).str.get_dummies().add_prefix('LOW_TERM_GPA_IND: ')
df_PARTTIME_TERM_IND = df['PARTTIME_TERM_IND'].apply(str).str.get_dummies().add_prefix('PARTTIME_TERM_IND: ')
df_NOT_REG_TERM_IND = df['NOT_REG_TERM_IND'].apply(str).str.get_dummies().add_prefix('NOT_REG_TERM_IND: ')
df_WITHDRWL_TERM_IND = df['WITHDRWL_TERM_IND'].apply(str).str.get_dummies().add_prefix('WITHDRWL_TERM_IND: ')
df_FULLTIME_TERM_IND = df['FULLTIME_TERM_IND'].apply(str).str.get_dummies().add_prefix('FULLTIME_TERM_IND: ')
df_OVR_12HR_TERM_IND = df['OVR_12HR_TERM_IND'].apply(str).str.get_dummies().add_prefix('OVR_12HR_TERM_IND: ')
df = pd.concat([df,df_JUNIOR_SENIOR_FLAGNO,df_ENRL_CNT,df_MAJORCOUNT,df_UF_CLASSNO,df_RESIDENCYNO,df_TERM_END_DT_SID_CATGRYNO,df_TERM_BEG_DT_CATGRYNO,df_LOW_TERM_GPA_IND,df_PARTTIME_TERM_IND,df_NOT_REG_TERM_IND,df_WITHDRWL_TERM_IND,df_FULLTIME_TERM_IND,df_OVR_12HR_TERM_IND],axis=1)
df = df.drop([
	'JUNIOR_SENIOR_FLAGNO',
 'ENRL_CNT',
 'MAJORCOUNT',
 'UF_CLASSNO',
 'RESIDENCYNO',
 'TERM_END_DT_SID_CATGRYNO',
 'TERM_BEG_DT_CATGRYNO',
 'LOW_TERM_GPA_IND',
 'PARTTIME_TERM_IND',
 'NOT_REG_TERM_IND',
 'WITHDRWL_TERM_IND',
 'FULLTIME_TERM_IND',
 'OVR_12HR_TERM_IND'
],axis=1)
 
Y = df['STUDENT_SUCCESSNO']
X = df.drop(['STUDENT_SUCCESSNO', 'TERM_END_DT_SID','TERM_SID','UNT_TAKEN_GPA'], axis= 1)
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=3234)
 
from sklearn.neighbors import KNeighborsClassifier as kNN
model = kNN(n_neighbors = 3, algorithm='auto', weights='distance')
model.fit(x_train,y_train)
y_pred = model.predict(x_test)
from sklearn.metrics import classification_report, confusion_matrix
print(classification_report(y_test, y_pred))

xtrainnp = np.array(x_train)
ytrainnp = np.array(y_train)
 
explainer = lime.lime_tabular.LimeTabularExplainer(xtrainnp,
                	feature_names=dfcolumn,
                	class_names=['1','2','3'],
                	verbose=True, mode='regression')
choosen_instance = x_test.iloc[[700]].values[0]
exp = explainer.explain_instance(choosen_instance, model.predict,num_features=22)
exp.show_in_notebook(show_all=False)
 
 
 
 
choosen_instance = x_test.iloc[[98]].values[0]
exp = explainer.explain_instance(choosen_instance, model.predict,num_features=22)
exp.show_in_notebook(show_all=False)
 





Save model:
from sklearn.externals import joblib
joblib.dump(model, "train_model.m")

Load model:
clf = joblib.load('train_model.m')

Pull 1 sample student who graduated in 4 years:
Sample1=df.iloc[-1:].drop(['STUDENT_SUCCESSNO'], axis= 1)
Pull 1 sample student who did not graduated :
Sample2=df.iloc[-2:-1].drop(['STUDENT_SUCCESSNO'], axis= 1)

Use our model to predict the two students:
Y_pred = model.predict(Sample1)
Y_pred = model.predict(Sample2)
 



















